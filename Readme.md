ğŸ§  Encoder-Only (BERT) â€” Customer Feedback Classification

This repository contains our implementation of Task 1 from Project 03: Fine-Tuning Transformer Architectures for Real-Time NLP Applications.
The task focuses on fine-tuning a pre-trained BERT model for sentiment classification on customer feedback data.
Each feedback entry is classified as Positive, Neutral, or Negative.

ğŸ¯ Objective

The goal was to adapt an Encoder-Only Transformer (BERT-base-uncased) to analyze customer feedback and predict sentiment polarity with high accuracy.

ğŸ“‚ Dataset

Source: Kaggle â€“ Customer Feedback Dataset

Classes: Positive | Neutral | Negative

Preprocessing: Cleaning, label encoding, tokenization with Hugging Face Tokenizer

âš™ï¸ Model Details
Component	Description
Architecture	BERT-base-uncased (Encoder-Only Transformer)
Task	Sequence Classification
Framework	PyTorch + Hugging Face Transformers
Training Epochs	8
Batch Size	16 (train) / 32 (eval)
Optimizer	AdamW (learning rate = 2e-5)
Evaluation Metrics	Accuracy and F1-Macro
ğŸ“Š Performance Summary
Metric	Value
Accuracy	1.00
F1-Macro	1.00

Confusion Matrix

[[9, 0, 0],
 [0, 1, 0],
 [0, 0, 11]]

ğŸ§© Example Predictions
Customer Feedback	Predicted Sentiment
â€œThe product quality is excellent and delivery was fast.â€	Positive
â€œItâ€™s okay, not great but not terrible either.â€	Neutral
â€œWorst service ever, Iâ€™m extremely disappointed.â€	Negative
ğŸ’» Usage

To run the Streamlit application locally:

pip install -r requirements.txt
streamlit run app.py

ğŸ§  Project Deliverables

âœ… Preprocessing and tokenization scripts

âœ… Training and validation pipeline

âœ… Evaluation (metrics + confusion matrix)

âœ… Example predictions in real-time via Streamlit interface

ğŸ‘¥ Contributors

Muhammad Usman Awan
Omima

ğŸ”— Repository Link

[https://github.com/ayan364/Encoder-Only-BERT-Sentiment-Classification](https://github.com/Usman-Ifty/Encoder-Only-BERT-Sentiment-Classification?tab=readme-ov-file)


